# -*- coding: utf-8 -*-
"""taxi_vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/137pi6WPGZ7UsUnndlyfYUr_dgFs_XoJz

# **Instalar dependencias**
"""

# Ejecutar para instalar todo lo necesario
!apt-get update -qq
!apt-get install -y -qq default-jdk graphviz

# Python packages
!pip install -q kaggle pyspark pandas pyarrow numpy scikit-learn tensorflow streamlit pyngrok folium streamlit-folium requests matplotlib

"""# **Subir kaggle.json**"""

# Subir el archivo kaggle.json mediante el di√°logo de archivos
from google.colab import files
print("Sube tu kaggle.json desde tu m√°quina cuando aparezca la ventana.")
uploaded = files.upload()

"""# **Configurar Kaggle y descargar dataset**"""

# Configurar Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Descargar dataset (es grande). Se descarga el ZIP completo.
!kaggle datasets download -d elemento/nyc-yellow-taxi-trip-data -p /content --unzip

"""# **Revisar archivos y listar archivos CSV**"""

import glob, os
files = glob.glob("/content/*.csv") + glob.glob("/content/data/*.csv") + glob.glob("/content/nyc_taxi_data/*.csv")
files = sorted(list(set(files)))
print(f"Archivos CSV encontrados ({len(files)}):")
for f in files[:50]:
    print(f)

"""# **Crear sample manejable y preparar para PySpark**"""

# Ajustar SAMPLE_ROWS (si se tiene suficiente RAM)
SAMPLE_ROWS = 300000   # recomendable 200k-500k para Colab

import pandas as pd
import glob
import os

# Lista de csv de trips (cambiar patr√≥n si difiere)
csv_files = [f for f in files if "zone" not in os.path.basename(f).lower()]

print("Archivos de trips a procesar:", len(csv_files))

rows_needed = SAMPLE_ROWS
dfs = []
for f in csv_files:
    if rows_needed <= 0:
        break
    try:
        df_chunk = pd.read_csv(f, nrows=rows_needed)
    except Exception as e:
        print("Error leyendo", f, e)
        continue
    dfs.append(df_chunk)
    rows_needed -= len(df_chunk)
    print(f"Le√≠do {len(df_chunk)} filas desde {os.path.basename(f)}; resto {rows_needed}")

if len(dfs)==0:
    raise SystemExit("No se pudieron leer archivos CSV ‚Äî revisa paths")

df_sample = pd.concat(dfs, ignore_index=True)
print("Total filas en sample:", len(df_sample))
df_sample.head()
# Guardar parquet para uso r√°pido
df_sample.to_parquet("nyc_sample.parquet", index=False)
print("Guardado: nyc_sample.parquet")

"""# **Descargar taxi_zone_lookup**"""

# Descarga de taxi+_zone_lookup
!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv -O taxi_zone_lookup.csv  # Fuente: repositorio oficial de NYC TLC

# Guardar taxi_zone_lookup.csv en un dataframe
zone_df = pd.read_csv("taxi_zone_lookup.csv")
print("Guardado taxi_zone_lookup.csv")

"""# **Iniciar PySpark (para ETL grande)**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("NYC_Taxi_ETL") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "8g") \
    .getOrCreate()

print(spark)

"""# **Preprocesamiento (limpieza y features)**"""

import pandas as pd
import numpy as np

df = pd.read_parquet("nyc_sample.parquet")
print("Columnas disponibles:", df.columns.tolist())

# Normalizar nombres m√°s comunes de columnas (var√≠an seg√∫n a√±o)
# Intentar detectar nombres para pickup datetime, location ids y coords
possible_pickup_cols = [c for c in df.columns if 'pickup' in c.lower() and 'datetime' in c.lower()]
possible_dropoff_cols = [c for c in df.columns if 'dropoff' in c.lower() and 'datetime' in c.lower()]

print("Pickup cols candidate:", possible_pickup_cols)
pickup_col = possible_pickup_cols[0] if possible_pickup_cols else None
dropoff_col = possible_dropoff_cols[0] if possible_dropoff_cols else None

# Location ID cols (PULocationID, PULocationID etc.)
# Note: los archivos antiguos podr√≠an no tener PULocationID, pero usar lat/lon directamente.
loc_cols = [c for c in df.columns if 'pulocationid' in c.lower() or 'pickup_locationid' in c.lower() or 'locationid' in c.lower()]
print("Location ID candidates:", loc_cols)
pu_loc_id_col = loc_cols[0] if loc_cols else None

# Coordenadas
lat_cols = [c for c in df.columns if 'pickup_lat' in c.lower() or 'pickup_latitude' in c.lower()]
lon_cols = [c for c in df.columns if 'pickup_lon' in c.lower() or 'pickup_longitude' in c.lower()]
pu_lat = lat_cols[0] if lat_cols else None
pu_lon = lon_cols[0] if lon_cols else None

# Renombramiento b√°sico a nombres can√≥nicos
if pickup_col:
    df.rename(columns={pickup_col: "tpep_pickup_datetime"}, inplace=True)
if dropoff_col:
    df.rename(columns={dropoff_col: "tpep_dropoff_datetime"}, inplace=True)

# Si la columna location ID se encuentra, renombrarla a PULocationID
if pu_loc_id_col:
    df.rename(columns={pu_loc_id_col: "PULocationID"}, inplace=True)
    # Convertir location id a entero (int) de ser posible
    try:
        df["PULocationID"] = df["PULocationID"].astype(int)
    except Exception as e:
        print(f"Warning: Could not convert 'PULocationID' to int: {e}, keeping original type.")

# Si PULocationID a√∫n no est√° en df.columns, crear sintaxis de lat/lon
if "PULocationID" not in df.columns and pu_lat and pu_lon:
    print("Warning: 'PULocationID' not found in the dataset. Creating synthetic 'PULocationID' from binned latitude and longitude.")
    # Agrupar la latitud y la longitud para crear una identificaci√≥n de zona pseudoaleatoria
    # Un tama√±o de bin de 0,01 grados de latitud equivale aproximadamente a 1,1 km.
    # Ajustar `round_to` para obtener la granularidad deseada. 2 significa 2 decimales.
    round_to = 2
    df["PULocationID"] = (df["pickup_latitude"].round(round_to)).astype(str) + "_" + \
                         (df["pickup_longitude"].round(round_to)).astype(str)
    print(f"Synthetic 'PULocationID' created using binned lat/lon (rounded to {round_to} decimal places).")
elif "PULocationID" not in df.columns:
    print("Warning: Neither 'PULocationID' nor valid latitude/longitude columns found to create a location identifier for grouping.")

# Si se encontraron coordenadas, ren√≥mbralas con nombres can√≥nicos
if pu_lat:
    df.rename(columns={pu_lat: "pickup_latitude"}, inplace=True)
if pu_lon:
    df.rename(columns={pu_lon: "pickup_longitude"}, inplace=True)


# Convertir datetimes y crear features temporales
df["tpep_pickup_datetime"] = pd.to_datetime(df["tpep_pickup_datetime"], errors="coerce")
df = df.dropna(subset=["tpep_pickup_datetime"])

df["pickup_hour"] = df["tpep_pickup_datetime"].dt.hour
df["pickup_day"] = df["tpep_pickup_datetime"].dt.dayofweek  # 0=Monday
df["pickup_date"] = df["tpep_pickup_datetime"].dt.date
df["pickup_month"] = df["tpep_pickup_datetime"].dt.month

# Duraci√≥n del recorrido (min) si existe parada
if "tpep_dropoff_datetime" in df.columns:
    df["tpep_dropoff_datetime"] = pd.to_datetime(df["tpep_dropoff_datetime"], errors="coerce")
    df["trip_duration_min"] = (df["tpep_dropoff_datetime"] - df["tpep_pickup_datetime"]).dt.total_seconds() / 60.0

# Recurrir a la distancia del viaje
if "trip_distance" not in df.columns and "Trip_distance" in df.columns:
    df.rename(columns={"Trip_distance":"trip_distance"}, inplace=True)

# Filtrar viajes v√°lidos
if "trip_duration_min" in df.columns:
    df = df[(df["trip_duration_min"]>0) & (df["trip_duration_min"]<180)]

# Gestionar el filtrado de ubicaciones nulas en funci√≥n de lo que est√© disponible
if "PULocationID" in df.columns:
    df = df[df["PULocationID"].notnull()]
elif "pickup_latitude" in df.columns and "pickup_longitude" in df.columns:
    print("No 'PULocationID' found, filtering based on 'pickup_latitude' and 'pickup_longitude' not being null.")
    df = df[df["pickup_latitude"].notnull() & df["pickup_longitude"].notnull()]
else:
    print("Warning: No 'PULocationID' or valid latitude/longitude columns found for location filtering.")


print("After preprocessing rows:", len(df))
df.head()
# Guardar el preprocesado (opcional)
df.to_parquet("nyc_preprocessed.parquet", index=False)

"""# **Crear variable objetivo: demanda por zona/hora**"""

# Agrupar para obtener demanda (viajes) por PULocationID, hora y dia
demand = df.groupby(["PULocationID", "pickup_date", "pickup_hour"]).size().reset_index(name="trips")
print("Total rows demand:", len(demand))
# Agregar agregados por zona+hora (media hist√≥rica) - √∫til para predecir demanda promedio por hora
demand_hourly = demand.groupby(["PULocationID", "pickup_hour"]).agg(
    demand_mean=("trips", "mean"),
    demand_median=("trips", "median"),
    demand_std=("trips", "std"),
    samples=("trips", "count")
).reset_index()
demand_hourly.head()
demand_hourly.to_parquet("demand_hourly.parquet", index=False)

"""# **Preparar features para ML (sk-learn + RandomForest)**"""

# Preparar la tabla de entrenamiento de la demanda original (zone+date+hour)
train_tbl = demand.copy()
# Agregar el atributo month de pickup_date (convertir)
train_tbl["month"] = pd.to_datetime(train_tbl["pickup_date"]).dt.month
train_tbl["dow"] = pd.to_datetime(train_tbl["pickup_date"]).dt.dayofweek

# Para el modelado, agregar por zone+hour+month+dow -> predecir viajes (contar)
X = train_tbl[["PULocationID", "pickup_hour", "month", "dow"]]
y = train_tbl["trips"]

# Codificar PULocationID como ordinal para mantener un pipeline simple
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

enc = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
X_enc = X.copy()
X_enc["PULocationID_enc"] = enc.fit_transform(X[["PULocationID"]])
X_enc = X_enc[["PULocationID_enc", "pickup_hour", "month", "dow"]]

X_train, X_test, y_train, y_test = train_test_split(X_enc, y, test_size=0.2, random_state=42)

rf = RandomForestRegressor(n_estimators=100, max_depth=12, n_jobs=-1, random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
print(f"RF RMSE: {rmse:.3f}  MAE: {mae:.3f}")

# Guadar el modelo y el encoder
import joblib
joblib.dump(rf, "rf_demand_model.joblib")
joblib.dump(enc, "ordinal_encoder.joblib")
print("Model and encoder saved.")

"""# **(Opcional) Modelo TensorFlow ligero**

(Opcional dado que RandomForest suele ser suficiente y m√°s interpretable).
"""

# Versi√≥n ligera con TF (opcional) ‚Äî entrenar una red densa peque√±a sobre mismas features
import tensorflow as tf
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_enc)

Xtr, Xte, ytr, yte = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(Xtr.shape[1],)),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer="adam", loss="mse", metrics=["mae"])
model.fit(Xtr, ytr, validation_data=(Xte, yte), epochs=5, batch_size=1024)

# Guardar modelo TF y scaler
model.save("tf_demand_model.keras")
joblib.dump(scaler, "scaler.joblib")
print("TF model and scaler saved.")

"""# **Preparar archivos necesarios para el Dashboard**"""

# Guardar demand_hourly y zone lookup en csv para lectura por Streamlit
demand_hourly.to_csv("demand_hourly.csv", index=False)
zone_df.to_csv("taxi_zone_lookup.csv", index=False)
# Guardar sample preprocessed (reducido) para heatmap
df[['PULocationID','pickup_date','pickup_hour','pickup_latitude','pickup_longitude']].to_parquet("nyc_for_dashboard.parquet", index=False)
print("Archivos para dashboard listos.")

"""# **Configurar clave OpenWeather y ngrok**"""

# Si se tiene OpenWeather API key, colocarla en esta secci√≥n (opcional). Si no, la app usar√° Open-Meteo.
OPENWEATHER_API_KEY = ""  # <--- pega key (si no, dejar vac√≠o)

# Ngrok token (opcional para exponer)
NGROK_TOKEN = "35XbxdR5DiJ1qqoI9IWiDr0iCAT_4V2W1m53QWEtmKP3Vjrsu"  # <--- pega ngrok authtoken para obtener enlace p√∫blico estable

"""# **Escribir el Streamlit app completo (app.py)**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import folium
# from folium.plugins import HeatMap
# from streamlit_folium import st_folium
# import joblib
# import requests
# import os
# 
# st.set_page_config(page_title="TaxiVision - Predicci√≥n de Demanda NYC", layout="wide")
# 
# 
# # 1. Cargar assets
# 
# @st.cache_data(show_spinner=True)
# def load_assets():
#     demand_hourly = pd.read_csv("demand_hourly.csv")
#     zone_lookup = pd.read_csv("taxi_zone_lookup.csv")
#     df_points = pd.read_parquet("nyc_for_dashboard.parquet")
#     model_rf = joblib.load("rf_demand_model.joblib")
#     encoder = joblib.load("ordinal_encoder.joblib")
#     # scaler/model TF optional
#     return demand_hourly, zone_lookup, df_points, model_rf, encoder
# 
# demand_hourly, zone_lookup, df_points, model_rf, encoder = load_assets()
# 
# st.title("TaxiVision - Predicci√≥n de Demanda NYC")
# 
# # Top layout: selector zona
# st.sidebar.header("Configuraci√≥n")
# boroughs = ["Todos"] + sorted(zone_lookup["Borough"].dropna().unique().tolist())
# sel_borough = st.sidebar.selectbox("Filtrar Borough", boroughs)
# 
# if sel_borough != "Todos":
#     zones_available = zone_lookup[zone_lookup["Borough"]==sel_borough]["Zone"].tolist()
# else:
#     zones_available = zone_lookup["Zone"].tolist()
# 
# zone_selected = st.selectbox("Seleccione Zona (por nombre):", zones_available, index=0)
# zone_row = zone_lookup[zone_lookup["Zone"]==zone_selected].iloc[0]
# zone_id = int(zone_row["LocationID"])
# 
# st.markdown(f"**Zona seleccionada:** {zone_selected} ‚Äî LocationID: {zone_id} ‚Äî Borough: {zone_row['Borough']}")
# 
# 
# # 2. Heatmap por hora
# 
# st.header("Mapa de calor (pickups)")
# 
# hour_sel = st.slider("Selecciona la hora (pickup)", 0, 23, 12)
# df_hour = df_points[df_points["pickup_hour"]==hour_sel].dropna(subset=["pickup_latitude","pickup_longitude"])
# heat_data = df_hour[["pickup_latitude","pickup_longitude"]].values.tolist()
# 
# m = folium.Map(location=[40.75, -73.98], zoom_start=11)
# if len(heat_data) > 0:
#     HeatMap(heat_data, radius=8, blur=10, max_zoom=13).add_to(m)
# st_folium(m, width=1000, height=550)
# 
# 
# # 3. Predicci√≥n por hora para zona seleccionada
# 
# st.header("Demanda Esperada por Hora (Predicci√≥n)")
# 
# # Construir input para las 24 horas
# hours = np.arange(0,24)
# # Usar month & dow como mediana de la selecci√≥n de fechas del conjunto de datos ‚Äî fex: tomar el mes m√°s com√∫n y dow
# # Pero lo m√°s sencillo: utilice month=6, dow=2 como marcadores de posici√≥n O calcule a partir de la distribuci√≥n del conjunto de datos
# common_month = int(demand_hourly.get('pickup_hour', pd.Series([1])).index[0]) if False else 6
# # Construir tabla
# input_df = pd.DataFrame({
#     "PULocationID": [zone_id]*24,
#     "pickup_hour": hours,
#     "month": [6]*24,
#     "dow": [2]*24
# })
# # Codificar y predecir con RF
# # Copiar entrada
# X_enc = input_df[["PULocationID"]].copy()
# 
# # Convertir todo a string, incluso NaN
# X_enc["PULocationID"] = X_enc["PULocationID"].astype(str)
# 
# # Reemplazar valores inv√°lidos por "-1" (string)
# X_enc["PULocationID"] = X_enc["PULocationID"].replace(["nan", "None", ""], "-1")
# 
# # Transformar
# X_enc["PULocationID_enc"] = encoder.transform(X_enc[["PULocationID"]])
# 
# X_enc = pd.DataFrame({
#     "PULocationID_enc": X_enc["PULocationID_enc"],
#     "pickup_hour": input_df["pickup_hour"],
#     "month": input_df["month"],
#     "dow": input_df["dow"]
# })
# 
# preds = model_rf.predict(X_enc)
# preds = np.clip(preds, a_min=0, a_max=None)
# 
# pred_df = pd.DataFrame({"pickup_hour": hours, "prediction": preds})
# # Suavizado con media m√≥vil para visualizaci√≥n
# pred_df["prediction_smooth"] = pred_df["prediction"].rolling(3, center=True, min_periods=1).mean()
# 
# # Trama
# st.line_chart(pred_df.set_index("pickup_hour")["prediction_smooth"])
# 
# # Mostrar tabla
# st.dataframe(pred_df.style.format({"prediction":"{:.1f}", "prediction_smooth":"{:.1f}"}))
# 
# 
# # 4. Comparar con hist√≥rico (demand_hourly)
# 
# st.header("Comparaci√≥n con demanda hist√≥rica (media por hora)")
# 
# hist = demand_hourly[demand_hourly["PULocationID"]==zone_id].sort_values("pickup_hour")
# if hist.empty:
#     st.info("No hay datos hist√≥ricos suficientes para esta zona en el sample.")
# else:
#     merged = pred_df.merge(hist[["pickup_hour","demand_mean"]], on="pickup_hour", how="left")
#     st.line_chart(merged.set_index("pickup_hour")[["prediction_smooth","demand_mean"]])
# 
# 
# # 5. Integraci√≥n m√≠nima de Clima
# 
# st.header("üå§ Clima (hist√≥rico) para la fecha seleccionada")
# 
# # Fecha selector basado en df_points
# dates = sorted(df_points["pickup_date"].unique())
# sel_date = st.selectbox("Selecciona fecha (para consultar clima):", dates, index=0)
# 
# use_openweather = os.environ.get("OPENWEATHER_API_KEY", "") != "" or False
# OW_KEY = os.environ.get("OPENWEATHER_API_KEY","")
# 
# if OW_KEY:
#     st.write("Usando OpenWeather (API key encontrada en variable de entorno).")
# else:
#     st.write("Usando Open-Meteo (no requiere key).")
# 
# def get_weather_openmete(date):
#     # Usar Open-Meteo (latitude/longitude de NYC)
#     url = (
#         "https://api.open-meteo.com/v1/forecast?"
#         "latitude=40.7128&longitude=-74.0060&hourly=temperature_2m,precipitation,"
#         f"timezone=America%2FNew_York&start_date={date}&end_date={date}"
#     )
#     r = requests.get(url, timeout=10)
#     return r.json()
# 
# def get_weather_openweather(date, key):
#     # OpenWeather One Call (historical) requiere timestamp unix por hora y cuenta PRO para historico extendido.
#     # Consulta de forecast para la fecha cercana (si no est√° disponible, fallback)
#     url = f"https://api.openweathermap.org/data/2.5/onecall?lat=40.7128&lon=-74.0060&exclude=minutely,daily,alerts&appid={key}&units=metric"
#     r = requests.get(url, timeout=10)
#     return r.json()
# 
# weather_json = None
# if OW_KEY:
#     try:
#         weather_json = get_weather_openweather(sel_date, OW_KEY)
#         # Extraer la temperatura por hora si est√° disponible
#         if 'hourly' in weather_json:
#             hrs = weather_json['hourly']
#             wdf = pd.DataFrame(hrs)[['dt','temp','rain']] if 'rain' in pd.DataFrame(hrs).columns else pd.DataFrame(hrs)[['dt','temp']]
#             # Convertir dt
#             wdf['Hora'] = pd.to_datetime(wdf['dt'], unit='s').dt.strftime("%Y-%m-%dT%H:%M")
#             wdf = wdf.rename(columns={'temp':'Temperatura (¬∞C)'}).fillna(0)
#         else:
#             weather_json = get_weather_openmete(sel_date)
#             hours = weather_json["hourly"]["time"]
#             temps = weather_json["hourly"]["temperature_2m"]
#             precip = weather_json["hourly"]["precipitation"]
#             wdf = pd.DataFrame({"Hora":hours, "Temperatura (¬∞C)":temps, "Precipitaci√≥n (mm)":precip})
#     except Exception as e:
#         st.warning("OpenWeather falla: usando Open-Meteo como fallback. Error: "+str(e))
#         weather_json = get_weather_openmete(sel_date)
#         hours = weather_json["hourly"]["time"]
#         temps = weather_json["hourly"]["temperature_2m"]
#         precip = weather_json["hourly"]["precipitation"]
#         wdf = pd.DataFrame({"Hora":hours, "Temperatura (¬∞C)":temps, "Precipitaci√≥n (mm)":precip})
# else:
#     try:
#         weather_json = get_weather_openmete(sel_date)
# 
#         # Validaci√≥n robusta de clave "hourly"
#         if ("hourly" not in weather_json or
#             "time" not in weather_json["hourly"] or
#             "temperature_2m" not in weather_json["hourly"]):
# 
#             st.warning("Open-Meteo no devolvi√≥ datos horarios para esta fecha.")
#             wdf = pd.DataFrame()
# 
#         else:
#             hours = weather_json["hourly"]["time"]
#             temps = weather_json["hourly"]["temperature_2m"]
#             precip = weather_json["hourly"].get("precipitation", [0]*len(hours))
# 
#             wdf = pd.DataFrame({
#                 "Hora": hours,
#                 "Temperatura (¬∞C)": temps,
#                 "Precipitaci√≥n (mm)": precip
#             })
# 
#     except Exception as e:
#         st.error("Error consultando Open-Meteo: " + str(e))
#         wdf = pd.DataFrame()
# 
# if not wdf.empty:
#     st.subheader("Valores horarios del clima")
#     st.dataframe(wdf)
#     st.line_chart(wdf.set_index("Hora")["Temperatura (¬∞C)"])
# 
# st.markdown("---")
# st.caption("Dashboard generado con sample del dataset. Para producci√≥n, usar todos los datos y re-entrenar modelo con features extendidos (clima, eventos, holidays).")

"""# **Ejecutar Streamlit y exponer con ngrok**"""

from pyngrok import ngrok, conf
import os, time, subprocess, sys

# NGROK_TOKEN = "35XbxdR5DiJ1qqoI9IWiDr0iCAT_4V2W1m53QWEtmKP3Vjrsu"  # Token ngrok
if NGROK_TOKEN:
    conf.get_default().auth_token = NGROK_TOKEN

# Terminar cualquier t√∫nel ngrok existente para evitar conflictos
ngrok.kill()
time.sleep(1) # A√±adir un peque√±o retraso despu√©s de eliminar los t√∫neles

# Iniciar streamlit en background
print("Iniciando Streamlit...")
# Ejecutar de forma que no bloquee la celda
get_ipython().system_raw("streamlit run app.py --server.port 8501 &\n")

# Abrir t√∫nel ngrok y mostrar la URL p√∫blica
time.sleep(3)
public_url = ngrok.connect(8501)
print("T√∫nel ngrok creado en:", public_url)
print("Abre la URL en tu navegador (puede tardar unos segundos en estar disponible).")
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalar dependencias**"
      ],
      "metadata": {
        "id": "KVOkExGSqP_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhToo0-ujgQL",
        "outputId": "2da52819-aee5-48f6-e33e-54fe5aa46300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "# Ejecutar en Colab: instala todo lo necesario\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq default-jdk graphviz\n",
        "\n",
        "# Python packages\n",
        "!pip install -q kaggle pyspark pandas pyarrow numpy scikit-learn tensorflow streamlit pyngrok folium streamlit-folium requests matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Subir kaggle.json**"
      ],
      "metadata": {
        "id": "7SidARbnrfrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar en Colab y subir el archivo kaggle.json mediante el di√°logo de archivos\n",
        "from google.colab import files\n",
        "print(\"Sube tu kaggle.json desde tu m√°quina cuando aparezca la ventana.\")\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "bqU8XP2QrefL",
        "outputId": "06ad51ad-b445-488a-d2b1-0f3b14a21e73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sube tu kaggle.json desde tu m√°quina cuando aparezca la ventana.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9a9a87f9-1018-4ced-97fd-19f8c155bb22\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9a9a87f9-1018-4ced-97fd-19f8c155bb22\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurar Kaggle y descargar dataset**"
      ],
      "metadata": {
        "id": "GksWBOaGrt8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar Kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Descargar dataset (es grande). Se descarga el ZIP completo.\n",
        "!kaggle datasets download -d elemento/nyc-yellow-taxi-trip-data -p /content --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y71LUBMjr6dx",
        "outputId": "235e9f32-66fa-4b47-a96e-44b314d93150"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data\n",
            "License(s): U.S. Government Works\n",
            "Downloading nyc-yellow-taxi-trip-data.zip to /content\n",
            " 99% 1.77G/1.78G [00:26<00:00, 14.0MB/s]\n",
            "100% 1.78G/1.78G [00:26<00:00, 71.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Revisar archivos y listar archivos CSV**"
      ],
      "metadata": {
        "id": "JYtKla6Uskeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "files = glob.glob(\"/content/*.csv\") + glob.glob(\"/content/data/*.csv\") + glob.glob(\"/content/nyc_taxi_data/*.csv\")\n",
        "files = sorted(list(set(files)))\n",
        "print(f\"Archivos CSV encontrados ({len(files)}):\")\n",
        "for f in files[:50]:\n",
        "    print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e_EJyamsr7e",
        "outputId": "ed5eee32-dac1-45dd-fcfa-994793729b24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos CSV encontrados (4):\n",
            "/content/yellow_tripdata_2015-01.csv\n",
            "/content/yellow_tripdata_2016-01.csv\n",
            "/content/yellow_tripdata_2016-02.csv\n",
            "/content/yellow_tripdata_2016-03.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Crear sample manejable y preparar para PySpark**"
      ],
      "metadata": {
        "id": "RIl1p6Tbsx2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar SAMPLE_ROWS si tu Colab tiene suficiente RAM\n",
        "SAMPLE_ROWS = 300000   # recomendable 200k-500k para Colab\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Lista de csv de trips (cambiar patr√≥n si difiere)\n",
        "csv_files = [f for f in files if \"zone\" not in os.path.basename(f).lower()]\n",
        "\n",
        "print(\"Archivos de trips a procesar:\", len(csv_files))\n",
        "\n",
        "rows_needed = SAMPLE_ROWS\n",
        "dfs = []\n",
        "for f in csv_files:\n",
        "    if rows_needed <= 0:\n",
        "        break\n",
        "    try:\n",
        "        df_chunk = pd.read_csv(f, nrows=rows_needed)\n",
        "    except Exception as e:\n",
        "        print(\"Error leyendo\", f, e)\n",
        "        continue\n",
        "    dfs.append(df_chunk)\n",
        "    rows_needed -= len(df_chunk)\n",
        "    print(f\"Le√≠do {len(df_chunk)} filas desde {os.path.basename(f)}; resto {rows_needed}\")\n",
        "\n",
        "if len(dfs)==0:\n",
        "    raise SystemExit(\"No se pudieron leer archivos CSV ‚Äî revisa paths\")\n",
        "\n",
        "df_sample = pd.concat(dfs, ignore_index=True)\n",
        "print(\"Total filas en sample:\", len(df_sample))\n",
        "df_sample.head()\n",
        "# Guardar parquet para uso r√°pido\n",
        "df_sample.to_parquet(\"nyc_sample.parquet\", index=False)\n",
        "print(\"Guardado: nyc_sample.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1uU4O5Js36d",
        "outputId": "088741ff-2d70-4845-fcf1-e3137a03c132"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos de trips a procesar: 4\n",
            "Le√≠do 300000 filas desde yellow_tripdata_2015-01.csv; resto 0\n",
            "Total filas en sample: 300000\n",
            "Guardado: nyc_sample.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descargar taxi_zone_lookup**"
      ],
      "metadata": {
        "id": "ksIsagJVtIJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga de taxi+_zone_lookup\n",
        "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv -O taxi_zone_lookup.csv  # Fuente: repositorio oficial de NYC TLC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbCutcPGt2x5",
        "outputId": "644a1cb7-aea6-4e16-b615-b3187a7b5ddd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-16 04:25:14--  https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.154.99.47, 18.154.99.220, 18.154.99.225, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.154.99.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12331 (12K) [text/csv]\n",
            "Saving to: ‚Äòtaxi_zone_lookup.csv‚Äô\n",
            "\n",
            "\rtaxi_zone_lookup.cs   0%[                    ]       0  --.-KB/s               \rtaxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-11-16 04:25:14 (13.0 MB/s) - ‚Äòtaxi_zone_lookup.csv‚Äô saved [12331/12331]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zone_df = pd.read_csv(\"taxi_zone_lookup.csv\")\n",
        "print(\"Guardado taxi_zone_lookup.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEQZWPOoylrP",
        "outputId": "4f4db8dd-c86a-4f55-d7f5-bb8edd922ece"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardado taxi_zone_lookup.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Iniciar PySpark (para ETL grande)**"
      ],
      "metadata": {
        "id": "u8XU6WvNu7FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NYC_Taxi_ETL\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(spark)"
      ],
      "metadata": {
        "id": "AMBchnX7vMS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3d41f9-099b-4965-c4d7-8dc4d49de623"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7ac92a2efd70>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento (limpieza y features)**"
      ],
      "metadata": {
        "id": "J3yQBoinvU7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_parquet(\"nyc_sample.parquet\")\n",
        "print(\"Columnas disponibles:\", df.columns.tolist())\n",
        "\n",
        "# Normalizar nombres m√°s comunes de columnas (var√≠an seg√∫n a√±o)\n",
        "# Intentar detectar nombres para pickup datetime, location ids y coords\n",
        "possible_pickup_cols = [c for c in df.columns if 'pickup' in c.lower() and 'datetime' in c.lower()]\n",
        "possible_dropoff_cols = [c for c in df.columns if 'dropoff' in c.lower() and 'datetime' in c.lower()]\n",
        "\n",
        "print(\"Pickup cols candidate:\", possible_pickup_cols)\n",
        "pickup_col = possible_pickup_cols[0] if possible_pickup_cols else None\n",
        "dropoff_col = possible_dropoff_cols[0] if possible_dropoff_cols else None\n",
        "\n",
        "# Location ID cols (PULocationID, PULocationID etc.)\n",
        "# Note: These older files might not have PULocationID, but use lat/lon directly.\n",
        "loc_cols = [c for c in df.columns if 'pulocationid' in c.lower() or 'pickup_locationid' in c.lower() or 'locationid' in c.lower()]\n",
        "print(\"Location ID candidates:\", loc_cols)\n",
        "pu_loc_id_col = loc_cols[0] if loc_cols else None\n",
        "\n",
        "# Coordinates\n",
        "lat_cols = [c for c in df.columns if 'pickup_lat' in c.lower() or 'pickup_latitude' in c.lower()]\n",
        "lon_cols = [c for c in df.columns if 'pickup_lon' in c.lower() or 'pickup_longitude' in c.lower()]\n",
        "pu_lat = lat_cols[0] if lat_cols else None\n",
        "pu_lon = lon_cols[0] if lon_cols else None\n",
        "\n",
        "# Basic rename to canonical names\n",
        "if pickup_col:\n",
        "    df.rename(columns={pickup_col: \"tpep_pickup_datetime\"}, inplace=True)\n",
        "if dropoff_col:\n",
        "    df.rename(columns={dropoff_col: \"tpep_dropoff_datetime\"}, inplace=True)\n",
        "\n",
        "# If a location ID column was found, rename it to PULocationID\n",
        "if pu_loc_id_col:\n",
        "    df.rename(columns={pu_loc_id_col: \"PULocationID\"}, inplace=True)\n",
        "    # Convert location id to int if possible\n",
        "    try:\n",
        "        df[\"PULocationID\"] = df[\"PULocationID\"].astype(int)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not convert 'PULocationID' to int: {e}, keeping original type.\")\n",
        "\n",
        "# If PULocationID is still not in df.columns, create a synthetic one from lat/lon\n",
        "if \"PULocationID\" not in df.columns and pu_lat and pu_lon:\n",
        "    print(\"Warning: 'PULocationID' not found in the dataset. Creating synthetic 'PULocationID' from binned latitude and longitude.\")\n",
        "    # Binning latitude and longitude to create a pseudo-zone ID\n",
        "    # A bin size of 0.01 degree latitude is roughly 1.1 km.\n",
        "    # Adjust `round_to` for desired granularity. 2 means 2 decimal places.\n",
        "    round_to = 2\n",
        "    df[\"PULocationID\"] = (df[\"pickup_latitude\"].round(round_to)).astype(str) + \"_\" + \\\n",
        "                         (df[\"pickup_longitude\"].round(round_to)).astype(str)\n",
        "    print(f\"Synthetic 'PULocationID' created using binned lat/lon (rounded to {round_to} decimal places).\")\n",
        "elif \"PULocationID\" not in df.columns:\n",
        "    print(\"Warning: Neither 'PULocationID' nor valid latitude/longitude columns found to create a location identifier for grouping.\")\n",
        "\n",
        "# If coordinates were found, rename them to canonical names\n",
        "if pu_lat:\n",
        "    df.rename(columns={pu_lat: \"pickup_latitude\"}, inplace=True)\n",
        "if pu_lon:\n",
        "    df.rename(columns={pu_lon: \"pickup_longitude\"}, inplace=True)\n",
        "\n",
        "\n",
        "# Convert datetimes y crear features temporales\n",
        "df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"tpep_pickup_datetime\"])\n",
        "\n",
        "df[\"pickup_hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
        "df[\"pickup_day\"] = df[\"tpep_pickup_datetime\"].dt.dayofweek  # 0=Monday\n",
        "df[\"pickup_date\"] = df[\"tpep_pickup_datetime\"].dt.date\n",
        "df[\"pickup_month\"] = df[\"tpep_pickup_datetime\"].dt.month\n",
        "\n",
        "# Trip duration (min) if dropoff exists\n",
        "if \"tpep_dropoff_datetime\" in df.columns:\n",
        "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
        "    df[\"trip_duration_min\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60.0\n",
        "\n",
        "# Trip distance fallback\n",
        "if \"trip_distance\" not in df.columns and \"Trip_distance\" in df.columns:\n",
        "    df.rename(columns={\"Trip_distance\":\"trip_distance\"}, inplace=True)\n",
        "\n",
        "# Filtrar viajes v√°lidos\n",
        "if \"trip_duration_min\" in df.columns:\n",
        "    df = df[(df[\"trip_duration_min\"]>0) & (df[\"trip_duration_min\"]<180)]\n",
        "\n",
        "# Handle location null filtering based on what's available\n",
        "if \"PULocationID\" in df.columns:\n",
        "    df = df[df[\"PULocationID\"].notnull()]\n",
        "elif \"pickup_latitude\" in df.columns and \"pickup_longitude\" in df.columns:\n",
        "    print(\"No 'PULocationID' found, filtering based on 'pickup_latitude' and 'pickup_longitude' not being null.\")\n",
        "    df = df[df[\"pickup_latitude\"].notnull() & df[\"pickup_longitude\"].notnull()]\n",
        "else:\n",
        "    print(\"Warning: No 'PULocationID' or valid latitude/longitude columns found for location filtering.\")\n",
        "\n",
        "\n",
        "print(\"After preprocessing rows:\", len(df))\n",
        "df.head()\n",
        "# Guardar el preprocesado (opcional)\n",
        "df.to_parquet(\"nyc_preprocessed.parquet\", index=False)"
      ],
      "metadata": {
        "id": "IBNuPKc4vX3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727b70f6-91b3-4cfe-ce4d-925f7d09feed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columnas disponibles: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'RateCodeID', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n",
            "Pickup cols candidate: ['tpep_pickup_datetime']\n",
            "Location ID candidates: []\n",
            "Warning: 'PULocationID' not found in the dataset. Creating synthetic 'PULocationID' from binned latitude and longitude.\n",
            "Synthetic 'PULocationID' created using binned lat/lon (rounded to 2 decimal places).\n",
            "After preprocessing rows: 299427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Crear variable objetivo: demanda por zona/hora**"
      ],
      "metadata": {
        "id": "ikGF3J9UwDAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar para obtener demanda (viajes) por PULocationID, hora y dia\n",
        "demand = df.groupby([\"PULocationID\", \"pickup_date\", \"pickup_hour\"]).size().reset_index(name=\"trips\")\n",
        "print(\"Total rows demand:\", len(demand))\n",
        "# Agregar agregados por zona+hora (media hist√≥rica) - √∫til para predecir demanda promedio por hora\n",
        "demand_hourly = demand.groupby([\"PULocationID\", \"pickup_hour\"]).agg(\n",
        "    demand_mean=(\"trips\", \"mean\"),\n",
        "    demand_median=(\"trips\", \"median\"),\n",
        "    demand_std=(\"trips\", \"std\"),\n",
        "    samples=(\"trips\", \"count\")\n",
        ").reset_index()\n",
        "demand_hourly.head()\n",
        "demand_hourly.to_parquet(\"demand_hourly.parquet\", index=False)"
      ],
      "metadata": {
        "id": "U00yFbmXwHxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a79920-7f1d-4da8-a609-35b50da51ede"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows demand: 40998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparar features para ML (sk-learn + RandomForest)**"
      ],
      "metadata": {
        "id": "xcg0V_42wktt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar la tabla de entrenamiento de la demanda original (zone+date+hour)\n",
        "train_tbl = demand.copy()\n",
        "# Agregar el atributo month de pickup_date (convertir)\n",
        "train_tbl[\"month\"] = pd.to_datetime(train_tbl[\"pickup_date\"]).dt.month\n",
        "train_tbl[\"dow\"] = pd.to_datetime(train_tbl[\"pickup_date\"]).dt.dayofweek\n",
        "\n",
        "# Para el modelado, agregar por zone+hour+month+dow -> predecir viajes (contar)\n",
        "X = train_tbl[[\"PULocationID\", \"pickup_hour\", \"month\", \"dow\"]]\n",
        "y = train_tbl[\"trips\"]\n",
        "\n",
        "# Codificar PULocationID como ordinal para mantener un pipeline simple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "X_enc = X.copy()\n",
        "X_enc[\"PULocationID_enc\"] = enc.fit_transform(X[[\"PULocationID\"]])\n",
        "X_enc = X_enc[[\"PULocationID_enc\", \"pickup_hour\", \"month\", \"dow\"]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_enc, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, max_depth=12, n_jobs=-1, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"RF RMSE: {rmse:.3f}  MAE: {mae:.3f}\")\n",
        "\n",
        "# Guadar el modelo y el encoder\n",
        "import joblib\n",
        "joblib.dump(rf, \"rf_demand_model.joblib\")\n",
        "joblib.dump(enc, \"ordinal_encoder.joblib\")\n",
        "print(\"Model and encoder saved.\")"
      ],
      "metadata": {
        "id": "hyLJXdQ2wvDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d90f843-4e33-4c3b-f114-116f6e23d466"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF RMSE: 6.029  MAE: 3.271\n",
            "Model and encoder saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(Opcional) Modelo TensorFlow ligero**\n",
        "\n",
        "(Opcional dado que RandomForest suele ser suficiente y m√°s interpretable)."
      ],
      "metadata": {
        "id": "40tMURWbxlL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Versi√≥n ligera con TF (opcional) ‚Äî entrenar una red densa peque√±a sobre mismas features\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_enc)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(Xtr.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "model.fit(Xtr, ytr, validation_data=(Xte, yte), epochs=5, batch_size=1024)\n",
        "\n",
        "# Guardar modelo TF y scaler\n",
        "model.save(\"tf_demand_model.keras\")\n",
        "joblib.dump(scaler, \"scaler.joblib\")\n",
        "print(\"TF model and scaler saved.\")"
      ],
      "metadata": {
        "id": "7NbKihzmxr7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53ace2e-92ae-4b9e-f182-eff2bc6ada1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 151.8728 - mae: 7.2052 - val_loss: 138.1688 - val_mae: 6.2627\n",
            "Epoch 2/5\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 134.0000 - mae: 6.2640 - val_loss: 118.0121 - val_mae: 6.0151\n",
            "Epoch 3/5\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 115.1368 - mae: 6.2334 - val_loss: 105.2029 - val_mae: 6.4898\n",
            "Epoch 4/5\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 102.1273 - mae: 6.6404 - val_loss: 101.7248 - val_mae: 6.6185\n",
            "Epoch 5/5\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 101.4446 - mae: 6.6751 - val_loss: 100.0667 - val_mae: 6.5467\n",
            "TF model and scaler saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparar archivos necesarios para el Dashboard**"
      ],
      "metadata": {
        "id": "6R05zDutyMCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar demand_hourly y zone lookup en csv para lectura por Streamlit\n",
        "demand_hourly.to_csv(\"demand_hourly.csv\", index=False)\n",
        "zone_df.to_csv(\"taxi_zone_lookup.csv\", index=False)\n",
        "# Guardar sample preprocessed (reducido) para heatmap\n",
        "df[['PULocationID','pickup_date','pickup_hour','pickup_latitude','pickup_longitude']].to_parquet(\"nyc_for_dashboard.parquet\", index=False)\n",
        "print(\"Archivos para dashboard listos.\")"
      ],
      "metadata": {
        "id": "Pl20RMdgySGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7d3cc2-62f5-480a-bdc6-d831f59493a1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos para dashboard listos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurar clave OpenWeather y ngrok**"
      ],
      "metadata": {
        "id": "66qrd2eEzV50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Si tienes OpenWeather API key, p√©gala aqu√≠ (opcional). Si no, el app usar√° Open-Meteo.\n",
        "OPENWEATHER_API_KEY = \"\"  # <--- pega aqu√≠ tu key (si no, deja vac√≠o)\n",
        "\n",
        "# Ngrok token (opcional para exponer)\n",
        "NGROK_TOKEN = \"35XbxdR5DiJ1qqoI9IWiDr0iCAT_4V2W1m53QWEtmKP3Vjrsu\"  # <--- pega tu ngrok authtoken si quieres enlace p√∫blico estable"
      ],
      "metadata": {
        "id": "aw0Eixz3zwzL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Escribir el Streamlit app completo (app.py)**"
      ],
      "metadata": {
        "id": "k6hVRWgqz_4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from streamlit_folium import st_folium\n",
        "import joblib\n",
        "import requests\n",
        "import os\n",
        "\n",
        "st.set_page_config(page_title=\"TaxiVision - Predicci√≥n de Demanda NYC\", layout=\"wide\")\n",
        "\n",
        "# Cargar assets\n",
        "@st.cache_data(show_spinner=True)\n",
        "def load_assets():\n",
        "    demand_hourly = pd.read_csv(\"demand_hourly.csv\")\n",
        "    zone_lookup = pd.read_csv(\"taxi_zone_lookup.csv\")\n",
        "    df_points = pd.read_parquet(\"nyc_for_dashboard.parquet\")\n",
        "    model_rf = joblib.load(\"rf_demand_model.joblib\")\n",
        "    encoder = joblib.load(\"ordinal_encoder.joblib\")\n",
        "    # scaler/model TF optional\n",
        "    return demand_hourly, zone_lookup, df_points, model_rf, encoder\n",
        "\n",
        "demand_hourly, zone_lookup, df_points, model_rf, encoder = load_assets()\n",
        "\n",
        "st.title(\"TaxiVision - Predicci√≥n de Demanda NYC\")\n",
        "\n",
        "# Top layout: selector zona\n",
        "st.sidebar.header(\"Configuraci√≥n\")\n",
        "boroughs = [\"Todos\"] + sorted(zone_lookup[\"Borough\"].dropna().unique().tolist())\n",
        "sel_borough = st.sidebar.selectbox(\"Filtrar Borough\", boroughs)\n",
        "\n",
        "if sel_borough != \"Todos\":\n",
        "    zones_available = zone_lookup[zone_lookup[\"Borough\"]==sel_borough][\"Zone\"].tolist()\n",
        "else:\n",
        "    zones_available = zone_lookup[\"Zone\"].tolist()\n",
        "\n",
        "zone_selected = st.selectbox(\"Seleccione Zona (por nombre):\", zones_available, index=0)\n",
        "zone_row = zone_lookup[zone_lookup[\"Zone\"]==zone_selected].iloc[0]\n",
        "zone_id = int(zone_row[\"LocationID\"])\n",
        "\n",
        "st.markdown(f\"**Zona seleccionada:** {zone_selected} ‚Äî LocationID: {zone_id} ‚Äî Borough: {zone_row['Borough']}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Heatmap por hora\n",
        "# -----------------------------\n",
        "st.header(\"Mapa de calor (pickups)\")\n",
        "\n",
        "hour_sel = st.slider(\"Selecciona la hora (pickup)\", 0, 23, 12)\n",
        "df_hour = df_points[df_points[\"pickup_hour\"]==hour_sel].dropna(subset=[\"pickup_latitude\",\"pickup_longitude\"])\n",
        "heat_data = df_hour[[\"pickup_latitude\",\"pickup_longitude\"]].values.tolist()\n",
        "\n",
        "m = folium.Map(location=[40.75, -73.98], zoom_start=11)\n",
        "if len(heat_data) > 0:\n",
        "    HeatMap(heat_data, radius=8, blur=10, max_zoom=13).add_to(m)\n",
        "st_folium(m, width=1000, height=550)\n",
        "\n",
        "# -----------------------------\n",
        "# Predicci√≥n por hora para zona seleccionada\n",
        "# -----------------------------\n",
        "st.header(\"Demanda Esperada por Hora (Predicci√≥n)\")\n",
        "\n",
        "# Construir input para las 24 horas\n",
        "hours = np.arange(0,24)\n",
        "# Usamos month & dow as median from dataset date selection ‚Äî fex: take most common month and dow\n",
        "# But simplest: use month=6, dow=2 as placeholders OR compute from dataset distribution\n",
        "common_month = int(demand_hourly.get('pickup_hour', pd.Series([1])).index[0]) if False else 6\n",
        "# Build table\n",
        "input_df = pd.DataFrame({\n",
        "    \"PULocationID\": [zone_id]*24,\n",
        "    \"pickup_hour\": hours,\n",
        "    \"month\": [6]*24,\n",
        "    \"dow\": [2]*24\n",
        "})\n",
        "# Encode and predict with RF\n",
        "# Copiar entrada\n",
        "X_enc = input_df[[\"PULocationID\"]].copy()\n",
        "\n",
        "# Convertir todo a string, incluso NaN\n",
        "X_enc[\"PULocationID\"] = X_enc[\"PULocationID\"].astype(str)\n",
        "\n",
        "# Reemplazar valores inv√°lidos por \"-1\" (string)\n",
        "X_enc[\"PULocationID\"] = X_enc[\"PULocationID\"].replace([\"nan\", \"None\", \"\"], \"-1\")\n",
        "\n",
        "# Transformar\n",
        "X_enc[\"PULocationID_enc\"] = encoder.transform(X_enc[[\"PULocationID\"]])\n",
        "\n",
        "X_enc = pd.DataFrame({\n",
        "    \"PULocationID_enc\": X_enc[\"PULocationID_enc\"],\n",
        "    \"pickup_hour\": input_df[\"pickup_hour\"],\n",
        "    \"month\": input_df[\"month\"],\n",
        "    \"dow\": input_df[\"dow\"]\n",
        "})\n",
        "\n",
        "preds = model_rf.predict(X_enc)\n",
        "preds = np.clip(preds, a_min=0, a_max=None)\n",
        "\n",
        "pred_df = pd.DataFrame({\"pickup_hour\": hours, \"prediction\": preds})\n",
        "# Smooth with rolling mean for visualization\n",
        "pred_df[\"prediction_smooth\"] = pred_df[\"prediction\"].rolling(3, center=True, min_periods=1).mean()\n",
        "\n",
        "# Plot\n",
        "st.line_chart(pred_df.set_index(\"pickup_hour\")[\"prediction_smooth\"])\n",
        "\n",
        "# Show table\n",
        "st.dataframe(pred_df.style.format({\"prediction\":\"{:.1f}\", \"prediction_smooth\":\"{:.1f}\"}))\n",
        "\n",
        "# -----------------------------\n",
        "# Comparar con hist√≥rico (demand_hourly)\n",
        "# -----------------------------\n",
        "st.header(\"Comparaci√≥n con demanda hist√≥rica (media por hora)\")\n",
        "\n",
        "hist = demand_hourly[demand_hourly[\"PULocationID\"]==zone_id].sort_values(\"pickup_hour\")\n",
        "if hist.empty:\n",
        "    st.info(\"No hay datos hist√≥ricos suficientes para esta zona en el sample.\")\n",
        "else:\n",
        "    merged = pred_df.merge(hist[[\"pickup_hour\",\"demand_mean\"]], on=\"pickup_hour\", how=\"left\")\n",
        "    st.line_chart(merged.set_index(\"pickup_hour\")[[\"prediction_smooth\",\"demand_mean\"]])\n",
        "\n",
        "# -----------------------------\n",
        "# Integraci√≥n m√≠nima de Clima\n",
        "# -----------------------------\n",
        "st.header(\"üå§ Clima (hist√≥rico) para la fecha seleccionada\")\n",
        "\n",
        "# Fecha selector basado en df_points\n",
        "dates = sorted(df_points[\"pickup_date\"].unique())\n",
        "sel_date = st.selectbox(\"Selecciona fecha (para consultar clima):\", dates, index=0)\n",
        "\n",
        "use_openweather = os.environ.get(\"OPENWEATHER_API_KEY\", \"\") != \"\" or False\n",
        "OW_KEY = os.environ.get(\"OPENWEATHER_API_KEY\",\"\")\n",
        "\n",
        "if OW_KEY:\n",
        "    st.write(\"Usando OpenWeather (API key encontrada en variable de entorno).\")\n",
        "else:\n",
        "    st.write(\"Usando Open-Meteo (no requiere key).\")\n",
        "\n",
        "def get_weather_openmete(date):\n",
        "    # usa Open-Meteo (latitude/longitude de NYC)\n",
        "    url = (\n",
        "        \"https://api.open-meteo.com/v1/forecast?\"\n",
        "        \"latitude=40.7128&longitude=-74.0060&hourly=temperature_2m,precipitation,\"\n",
        "        f\"timezone=America%2FNew_York&start_date={date}&end_date={date}\"\n",
        "    )\n",
        "    r = requests.get(url, timeout=10)\n",
        "    return r.json()\n",
        "\n",
        "def get_weather_openweather(date, key):\n",
        "    # OpenWeather One Call (historical) requiere timestamp unix por hora y cuenta PRO para historico extendido.\n",
        "    # Aqu√≠ hacemos una simple consulta de forecast para la fecha cercana (si no est√° disponible, fallback)\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/onecall?lat=40.7128&lon=-74.0060&exclude=minutely,daily,alerts&appid={key}&units=metric\"\n",
        "    r = requests.get(url, timeout=10)\n",
        "    return r.json()\n",
        "\n",
        "weather_json = None\n",
        "if OW_KEY:\n",
        "    try:\n",
        "        weather_json = get_weather_openweather(sel_date, OW_KEY)\n",
        "        # Extract hourly temp if available\n",
        "        if 'hourly' in weather_json:\n",
        "            hrs = weather_json['hourly']\n",
        "            wdf = pd.DataFrame(hrs)[['dt','temp','rain']] if 'rain' in pd.DataFrame(hrs).columns else pd.DataFrame(hrs)[['dt','temp']]\n",
        "            # convert dt\n",
        "            wdf['Hora'] = pd.to_datetime(wdf['dt'], unit='s').dt.strftime(\"%Y-%m-%dT%H:%M\")\n",
        "            wdf = wdf.rename(columns={'temp':'Temperatura (¬∞C)'}).fillna(0)\n",
        "        else:\n",
        "            weather_json = get_weather_openmete(sel_date)\n",
        "            hours = weather_json[\"hourly\"][\"time\"]\n",
        "            temps = weather_json[\"hourly\"][\"temperature_2m\"]\n",
        "            precip = weather_json[\"hourly\"][\"precipitation\"]\n",
        "            wdf = pd.DataFrame({\"Hora\":hours, \"Temperatura (¬∞C)\":temps, \"Precipitaci√≥n (mm)\":precip})\n",
        "    except Exception as e:\n",
        "        st.warning(\"OpenWeather falla: usando Open-Meteo como fallback. Error: \"+str(e))\n",
        "        weather_json = get_weather_openmete(sel_date)\n",
        "        hours = weather_json[\"hourly\"][\"time\"]\n",
        "        temps = weather_json[\"hourly\"][\"temperature_2m\"]\n",
        "        precip = weather_json[\"hourly\"][\"precipitation\"]\n",
        "        wdf = pd.DataFrame({\"Hora\":hours, \"Temperatura (¬∞C)\":temps, \"Precipitaci√≥n (mm)\":precip})\n",
        "else:\n",
        "    try:\n",
        "        weather_json = get_weather_openmete(sel_date)\n",
        "\n",
        "        # Validaci√≥n robusta de clave \"hourly\"\n",
        "        if (\"hourly\" not in weather_json or\n",
        "            \"time\" not in weather_json[\"hourly\"] or\n",
        "            \"temperature_2m\" not in weather_json[\"hourly\"]):\n",
        "\n",
        "            st.warning(\"Open-Meteo no devolvi√≥ datos horarios para esta fecha.\")\n",
        "            wdf = pd.DataFrame()\n",
        "\n",
        "        else:\n",
        "            hours = weather_json[\"hourly\"][\"time\"]\n",
        "            temps = weather_json[\"hourly\"][\"temperature_2m\"]\n",
        "            precip = weather_json[\"hourly\"].get(\"precipitation\", [0]*len(hours))\n",
        "\n",
        "            wdf = pd.DataFrame({\n",
        "                \"Hora\": hours,\n",
        "                \"Temperatura (¬∞C)\": temps,\n",
        "                \"Precipitaci√≥n (mm)\": precip\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(\"Error consultando Open-Meteo: \" + str(e))\n",
        "        wdf = pd.DataFrame()\n",
        "\n",
        "if not wdf.empty:\n",
        "    st.subheader(\"Valores horarios del clima\")\n",
        "    st.dataframe(wdf)\n",
        "    st.line_chart(wdf.set_index(\"Hora\")[\"Temperatura (¬∞C)\"])\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Dashboard generado con sample del dataset. Para producci√≥n, usar todos los datos y re-entrenar modelo con features extendidos (clima, eventos, holidays).\")"
      ],
      "metadata": {
        "id": "bTL6ueTH0LL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b079a4-745d-4c73-b766-e7c0cba05dae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CATEGOR√çAS DEL ENCODER:\", enc.categories_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gbUw_ybC76I",
        "outputId": "a6ca0733-76eb-4dce-a790-31b173435873"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CATEGOR√çAS DEL ENCODER: [array(['0.0_0.0', '18.63_-76.66', '19.47_-87.45', '30.13_-77.97',\n",
            "       '35.03_-70.05', '35.87_-71.06', '4.79_-73.95', '40.06_-73.34',\n",
            "       '40.12_-72.45', '40.17_-75.5', '40.27_-74.03', '40.47_-74.46',\n",
            "       '40.54_-74.16', '40.55_-74.2', '40.55_-74.3', '40.56_-74.05',\n",
            "       '40.57_-74.23', '40.58_-73.74', '40.58_-73.96', '40.58_-73.99',\n",
            "       '40.59_-73.75', '40.59_-73.79', '40.59_-73.93', '40.59_-73.97',\n",
            "       '40.59_-73.99', '40.59_-74.25', '40.61_-73.76', '40.61_-73.78',\n",
            "       '40.61_-73.91', '40.61_-73.96', '40.61_-73.98', '40.61_-74.0',\n",
            "       '40.61_-74.01', '40.61_-74.03', '40.61_-74.04', '40.61_-74.1',\n",
            "       '40.62_-73.78', '40.62_-73.85', '40.62_-73.94', '40.62_-73.97',\n",
            "       '40.62_-74.0', '40.62_-74.02', '40.62_-74.03', '40.62_-74.04',\n",
            "       '40.62_-74.27', '40.63_-73.77', '40.63_-73.78', '40.63_-73.79',\n",
            "       '40.63_-73.8', '40.63_-73.88', '40.63_-73.9', '40.63_-73.93',\n",
            "       '40.63_-73.94', '40.63_-73.95', '40.63_-73.96', '40.63_-73.97',\n",
            "       '40.63_-73.98', '40.63_-74.0', '40.63_-74.01', '40.63_-74.02',\n",
            "       '40.63_-74.03', '40.63_-74.1', '40.63_-74.12', '40.64_-73.78',\n",
            "       '40.64_-73.79', '40.64_-73.8', '40.64_-73.89', '40.64_-73.9',\n",
            "       '40.64_-73.92', '40.64_-73.93', '40.64_-73.94', '40.64_-73.95',\n",
            "       '40.64_-73.96', '40.64_-73.97', '40.64_-73.98', '40.64_-74.01',\n",
            "       '40.64_-74.02', '40.64_-74.07', '40.64_-74.14', '40.65_-73.78',\n",
            "       '40.65_-73.79', '40.65_-73.8', '40.65_-73.81', '40.65_-73.88',\n",
            "       '40.65_-73.93', '40.65_-73.95', '40.65_-73.96', '40.65_-73.97',\n",
            "       '40.65_-73.98', '40.65_-73.99', '40.65_-74.0', '40.65_-74.01',\n",
            "       '40.65_-74.02', '40.66_-73.51', '40.66_-73.75', '40.66_-73.79',\n",
            "       '40.66_-73.8', '40.66_-73.81', '40.66_-73.82', '40.66_-73.88',\n",
            "       '40.66_-73.89', '40.66_-73.93', '40.66_-73.94', '40.66_-73.95',\n",
            "       '40.66_-73.96', '40.66_-73.98', '40.66_-73.99', '40.66_-74.0',\n",
            "       '40.66_-74.01', '40.67_-73.62', '40.67_-73.76', '40.67_-73.77',\n",
            "       '40.67_-73.78', '40.67_-73.79', '40.67_-73.8', '40.67_-73.82',\n",
            "       '40.67_-73.83', '40.67_-73.84', '40.67_-73.91', '40.67_-73.93',\n",
            "       '40.67_-73.94', '40.67_-73.95', '40.67_-73.96', '40.67_-73.97',\n",
            "       '40.67_-73.98', '40.67_-73.99', '40.67_-74.0', '40.67_-74.01',\n",
            "       '40.67_-74.1', '40.68_-73.69', '40.68_-73.71', '40.68_-73.79',\n",
            "       '40.68_-73.8', '40.68_-73.81', '40.68_-73.83', '40.68_-73.84',\n",
            "       '40.68_-73.85', '40.68_-73.86', '40.68_-73.89', '40.68_-73.91',\n",
            "       '40.68_-73.92', '40.68_-73.93', '40.68_-73.94', '40.68_-73.95',\n",
            "       '40.68_-73.96', '40.68_-73.97', '40.68_-73.98', '40.68_-73.99',\n",
            "       '40.68_-74.0', '40.68_-74.01', '40.69_-73.63', '40.69_-73.74',\n",
            "       '40.69_-73.81', '40.69_-73.82', '40.69_-73.83', '40.69_-73.84',\n",
            "       '40.69_-73.85', '40.69_-73.86', '40.69_-73.91', '40.69_-73.92',\n",
            "       '40.69_-73.93', '40.69_-73.94', '40.69_-73.95', '40.69_-73.96',\n",
            "       '40.69_-73.97', '40.69_-73.98', '40.69_-73.99', '40.69_-74.0',\n",
            "       '40.69_-74.07', '40.69_-74.18', '40.69_-74.19', '40.6_-73.75',\n",
            "       '40.6_-73.76', '40.6_-73.77', '40.6_-73.78', '40.6_-73.9',\n",
            "       '40.6_-73.91', '40.6_-73.99', '40.6_-74.0', '40.6_-74.1',\n",
            "       '40.6_-74.23', '40.71_-73.69', '40.71_-73.78', '40.71_-73.79',\n",
            "       '40.71_-73.8', '40.71_-73.81', '40.71_-73.82', '40.71_-73.83',\n",
            "       '40.71_-73.84', '40.71_-73.85', '40.71_-73.86', '40.71_-73.9',\n",
            "       '40.71_-73.91', '40.71_-73.92', '40.71_-73.93', '40.71_-73.94',\n",
            "       '40.71_-73.95', '40.71_-73.96', '40.71_-73.97', '40.71_-73.98',\n",
            "       '40.71_-73.99', '40.71_-74.0', '40.71_-74.01', '40.71_-74.02',\n",
            "       '40.71_-74.04', '40.71_-74.05', '40.71_-74.15', '40.71_-74.17',\n",
            "       '40.72_-73.59', '40.72_-73.75', '40.72_-73.8', '40.72_-73.81',\n",
            "       '40.72_-73.82', '40.72_-73.83', '40.72_-73.84', '40.72_-73.85',\n",
            "       '40.72_-73.86', '40.72_-73.87', '40.72_-73.89', '40.72_-73.9',\n",
            "       '40.72_-73.91', '40.72_-73.94', '40.72_-73.95', '40.72_-73.96',\n",
            "       '40.72_-73.97', '40.72_-73.98', '40.72_-73.99', '40.72_-74.0',\n",
            "       '40.72_-74.01', '40.72_-74.02', '40.72_-74.03', '40.72_-74.04',\n",
            "       '40.72_-74.06', '40.72_-74.1', '40.72_-74.14', '40.73_-73.75',\n",
            "       '40.73_-73.79', '40.73_-73.81', '40.73_-73.82', '40.73_-73.83',\n",
            "       '40.73_-73.84', '40.73_-73.85', '40.73_-73.86', '40.73_-73.87',\n",
            "       '40.73_-73.88', '40.73_-73.89', '40.73_-73.9', '40.73_-73.91',\n",
            "       '40.73_-73.92', '40.73_-73.93', '40.73_-73.94', '40.73_-73.95',\n",
            "       '40.73_-73.96', '40.73_-73.97', '40.73_-73.98', '40.73_-73.99',\n",
            "       '40.73_-74.0', '40.73_-74.01', '40.73_-74.02', '40.73_-74.03',\n",
            "       '40.73_-74.04', '40.73_-74.05', '40.73_-74.06', '40.73_-74.1',\n",
            "       '40.73_-74.28', '40.74_-73.66', '40.74_-73.72', '40.74_-73.79',\n",
            "       '40.74_-73.84', '40.74_-73.85', '40.74_-73.86', '40.74_-73.87',\n",
            "       '40.74_-73.88', '40.74_-73.89', '40.74_-73.9', '40.74_-73.91',\n",
            "       '40.74_-73.92', '40.74_-73.93', '40.74_-73.94', '40.74_-73.95',\n",
            "       '40.74_-73.96', '40.74_-73.97', '40.74_-73.98', '40.74_-73.99',\n",
            "       '40.74_-74.0', '40.74_-74.01', '40.74_-74.03', '40.74_-74.04',\n",
            "       '40.74_-74.09', '40.75_-73.58', '40.75_-73.59', '40.75_-73.71',\n",
            "       '40.75_-73.81', '40.75_-73.83', '40.75_-73.85', '40.75_-73.86',\n",
            "       '40.75_-73.87', '40.75_-73.88', '40.75_-73.89', '40.75_-73.9',\n",
            "       '40.75_-73.91', '40.75_-73.92', '40.75_-73.93', '40.75_-73.94',\n",
            "       '40.75_-73.95', '40.75_-73.96', '40.75_-73.97', '40.75_-73.98',\n",
            "       '40.75_-73.99', '40.75_-74.0', '40.75_-74.01', '40.75_-74.02',\n",
            "       '40.75_-74.03', '40.75_-74.04', '40.75_-74.06', '40.76_-73.53',\n",
            "       '40.76_-73.77', '40.76_-73.8', '40.76_-73.82', '40.76_-73.83',\n",
            "       '40.76_-73.85', '40.76_-73.86', '40.76_-73.87', '40.76_-73.88',\n",
            "       '40.76_-73.89', '40.76_-73.9', '40.76_-73.91', '40.76_-73.92',\n",
            "       '40.76_-73.93', '40.76_-73.94', '40.76_-73.95', '40.76_-73.96',\n",
            "       '40.76_-73.97', '40.76_-73.98', '40.76_-73.99', '40.76_-74.0',\n",
            "       '40.76_-74.01', '40.76_-74.02', '40.76_-74.03', '40.77_-73.53',\n",
            "       '40.77_-73.75', '40.77_-73.82', '40.77_-73.83', '40.77_-73.84',\n",
            "       '40.77_-73.86', '40.77_-73.87', '40.77_-73.88', '40.77_-73.89',\n",
            "       '40.77_-73.9', '40.77_-73.91', '40.77_-73.92', '40.77_-73.93',\n",
            "       '40.77_-73.94', '40.77_-73.95', '40.77_-73.96', '40.77_-73.97',\n",
            "       '40.77_-73.98', '40.77_-73.99', '40.77_-74.0', '40.77_-74.01',\n",
            "       '40.77_-74.02', '40.77_-74.03', '40.77_-74.04', '40.77_-74.14',\n",
            "       '40.78_-73.53', '40.78_-73.59', '40.78_-73.87', '40.78_-73.9',\n",
            "       '40.78_-73.91', '40.78_-73.92', '40.78_-73.93', '40.78_-73.94',\n",
            "       '40.78_-73.95', '40.78_-73.96', '40.78_-73.97', '40.78_-73.98',\n",
            "       '40.78_-73.99', '40.78_-74.0', '40.78_-74.01', '40.78_-74.05',\n",
            "       '40.78_-74.06', '40.79_-73.72', '40.79_-73.87', '40.79_-73.89',\n",
            "       '40.79_-73.93', '40.79_-73.94', '40.79_-73.95', '40.79_-73.96',\n",
            "       '40.79_-73.97', '40.79_-73.98', '40.79_-74.02', '40.7_-73.8',\n",
            "       '40.7_-73.81', '40.7_-73.82', '40.7_-73.83', '40.7_-73.84',\n",
            "       '40.7_-73.86', '40.7_-73.89', '40.7_-73.9', '40.7_-73.91',\n",
            "       '40.7_-73.92', '40.7_-73.93', '40.7_-73.94', '40.7_-73.95',\n",
            "       '40.7_-73.96', '40.7_-73.97', '40.7_-73.98', '40.7_-73.99',\n",
            "       '40.7_-74.0', '40.7_-74.01', '40.7_-74.02', '40.7_-74.18',\n",
            "       '40.7_-74.3', '40.81_-73.7', '40.81_-73.85', '40.81_-73.9',\n",
            "       '40.81_-73.91', '40.81_-73.92', '40.81_-73.93', '40.81_-73.94',\n",
            "       '40.81_-73.95', '40.81_-73.96', '40.81_-73.97', '40.81_-73.98',\n",
            "       '40.82_-73.84', '40.82_-73.86', '40.82_-73.88', '40.82_-73.89',\n",
            "       '40.82_-73.9', '40.82_-73.91', '40.82_-73.92', '40.82_-73.93',\n",
            "       '40.82_-73.94', '40.82_-73.95', '40.82_-73.96', '40.82_-74.03',\n",
            "       '40.82_-74.25', '40.83_-73.7', '40.83_-73.83', '40.83_-73.85',\n",
            "       '40.83_-73.86', '40.83_-73.87', '40.83_-73.88', '40.83_-73.91',\n",
            "       '40.83_-73.92', '40.83_-73.93', '40.83_-73.94', '40.83_-73.95',\n",
            "       '40.83_-74.24', '40.84_-73.83', '40.84_-73.85', '40.84_-73.86',\n",
            "       '40.84_-73.88', '40.84_-73.89', '40.84_-73.9', '40.84_-73.91',\n",
            "       '40.84_-73.92', '40.84_-73.93', '40.84_-73.94', '40.84_-73.95',\n",
            "       '40.84_-73.96', '40.84_-73.98', '40.84_-74.19', '40.85_-73.79',\n",
            "       '40.85_-73.85', '40.85_-73.89', '40.85_-73.9', '40.85_-73.91',\n",
            "       '40.85_-73.92', '40.85_-73.93', '40.85_-73.94', '40.85_-73.95',\n",
            "       '40.85_-73.96', '40.85_-74.01', '40.85_-74.07', '40.85_-74.08',\n",
            "       '40.85_-74.18', '40.86_-73.82', '40.86_-73.83', '40.86_-73.88',\n",
            "       '40.86_-73.89', '40.86_-73.9', '40.86_-73.91', '40.86_-73.92',\n",
            "       '40.86_-73.93', '40.86_-73.94', '40.87_-73.88', '40.87_-73.89',\n",
            "       '40.87_-73.9', '40.87_-73.91', '40.87_-73.92', '40.87_-73.93',\n",
            "       '40.87_-73.96', '40.87_-73.97', '40.88_-73.85', '40.88_-73.86',\n",
            "       '40.88_-73.87', '40.88_-73.88', '40.88_-73.89', '40.88_-73.9',\n",
            "       '40.88_-73.91', '40.88_-74.0', '40.89_-73.61', '40.89_-73.86',\n",
            "       '40.89_-73.91', '40.89_-74.02', '40.89_-74.05', '40.8_-73.83',\n",
            "       '40.8_-73.92', '40.8_-73.93', '40.8_-73.94', '40.8_-73.95',\n",
            "       '40.8_-73.96', '40.8_-73.97', '40.8_-73.98', '40.8_-74.0',\n",
            "       '40.8_-74.06', '40.8_-74.15', '40.91_-73.78', '40.91_-73.79',\n",
            "       '40.91_-73.85', '40.91_-73.87', '40.91_-73.91', '40.91_-74.17',\n",
            "       '40.92_-74.09', '40.93_-73.76', '40.93_-73.84', '40.93_-74.1',\n",
            "       '40.94_-73.82', '40.94_-73.85', '40.94_-74.12', '40.98_-73.67',\n",
            "       '40.99_-69.6', '40.99_-74.23', '40.9_-73.86', '40.9_-74.03',\n",
            "       '41.02_-73.72', '41.06_-73.43', '41.06_-73.86', '41.0_-73.75',\n",
            "       '41.0_-80.59', '41.11_-73.7', '41.15_-73.67', '41.15_-73.93'],\n",
            "      dtype=object)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejecutar Streamlit y exponer con ngrok**"
      ],
      "metadata": {
        "id": "qcuhEhPX0Zqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import os, time, subprocess, sys\n",
        "\n",
        "# NGROK_TOKEN = \"35XbxdR5DiJ1qqoI9IWiDr0iCAT_4V2W1m53QWEtmKP3Vjrsu\"  # Token ngrok\n",
        "if NGROK_TOKEN:\n",
        "    conf.get_default().auth_token = NGROK_TOKEN\n",
        "\n",
        "# Terminar cualquier t√∫nel ngrok existente para evitar conflictos\n",
        "ngrok.kill()\n",
        "time.sleep(1) # Add a small delay after killing tunnels\n",
        "\n",
        "# Iniciar streamlit en background\n",
        "print(\"Iniciando Streamlit...\")\n",
        "# Ejecutar de forma que no bloquee la celda\n",
        "get_ipython().system_raw(\"streamlit run app.py --server.port 8501 &\\n\")\n",
        "\n",
        "# Abrir t√∫nel ngrok y mostrar la URL p√∫blica\n",
        "time.sleep(3)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"T√∫nel ngrok creado en:\", public_url)\n",
        "print(\"Abre la URL en tu navegador (puede tardar unos segundos en estar disponible).\")"
      ],
      "metadata": {
        "id": "K_hGGG3t1SPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "833885bd-2b9d-4521-fb1b-8efa0cba024d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando Streamlit...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-11-16T05:22:34+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T√∫nel ngrok creado en: NgrokTunnel: \"https://illa-cerebrational-appellatively.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "Abre la URL en tu navegador (puede tardar unos segundos en estar disponible).\n"
          ]
        }
      ]
    }
  ]
}